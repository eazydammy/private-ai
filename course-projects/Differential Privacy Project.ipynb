{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install PySyft if not available (e.g on cloud)\n",
    "# pip install syft\n",
    "\n",
    "# import required libraries\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data set\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainset.test_data\n",
    "\n",
    "train_data = mnist_trainset.train_data\n",
    "train_targets = mnist_trainset.train_labels\n",
    "\n",
    "test_data = mnist_trainset.test_data\n",
    "test_targets = mnist_trainset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to split training data into sub-datasets for a number of teachers\n",
    "# returns nt sub-datasets to train the each teacher model\n",
    "# TODO: split into train and test \n",
    "def splitTrainData(train_data, num_teachers):\n",
    "    teachers_train_data = []\n",
    "    p, q = divmod(len(train_data), num_teachers)\n",
    "    split_indices = list((train_data[i * p + min(i, q):(i + 1) * p + min(i + 1, q)] for i in range(num_teachers)))\n",
    "    for j in range(len(split_indices)):\n",
    "        subset_j = Subset(train_data, split_indices[j])\n",
    "        loader_j = torch.utils.data.DataLoader(subset_j, batch_size=64, shuffle=True)\n",
    "        teachers_train_data.append(loader_j)\n",
    "    return teachers_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to take a chunk of the test data as private dataset\n",
    "# returns reduced test data and private dataset\n",
    "def createPrivateDataset(test_data):\n",
    "    reduced_test_data = []\n",
    "    private_data = []\n",
    "    \n",
    "    return reduced_test_data, private_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class to build linear classifier models for each teacher\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to train model given train and test datasets\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps += 1\n",
    "            images.resize_(images.size()[0], 784)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                      \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "                \n",
    "                running_loss = 0\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to validate model using the reduced test data set\n",
    "def validation(model, testloader, criterion):\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.resize_(images.size()[0], 784)\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "        ## Calculating the accuracy \n",
    "        # Model's output is log-softmax, take exponential to get the probabilities\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(1)[1])\n",
    "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to perform PATE analysis\n",
    "def runPateAnalysis():\n",
    "    data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=indices, noise_eps=0.1, delta=1e-5)\n",
    "    print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "    print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers_train_data = splitTrainData(train_data, num_teachers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher1_trainloader = teachers_train_data[1]\n",
    "teacher1_trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher1_model = Classifier(784, 10, [512, 256, 128], drop_p=0.25)\n",
    "teacher1_train_data = teachers_train_data[1]\n",
    "teacher1_test_data = \n",
    "teacher1_trainloader = \n",
    "teacher1_testloader = \n",
    "teacher1_criterion = nn.NLLLoss()\n",
    "teacher1_optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
